<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Person of Interest: How I built a Facial Recognition Machine Learning model. ¬∑ Tarka Labs - Tarka Labs</title><meta name="gridsome:hash" content="0cf23ec60406531217e5ce675f99a5997c846f10"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.23"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" name="viewport" content="width=device-width, initial-scale=1.0"><meta data-vue-tag="ssr" property="og:title" content="Person of Interest: How I built a Facial Recognition Machine Learning model. ¬∑ Tarka Labs"><meta data-vue-tag="ssr" property="og:description" content="Person of Interest: How I built a Facial Recognition Machine Learning model."><meta data-vue-tag="ssr" property="og:image" content="/assets/img/1.41951ccc.webp"><meta data-vue-tag="ssr" property="og:image:height" content="627"><meta data-vue-tag="ssr" property="og:image:width" content="1200"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/assets/static/favicon.ce0531f.92f6d0f06feb6aec5e019521ff224fd8.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/assets/static/favicon.ac8d93a.92f6d0f06feb6aec5e019521ff224fd8.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/assets/static/favicon.b9532cc.92f6d0f06feb6aec5e019521ff224fd8.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/assets/static/favicon.f22e9f3.92f6d0f06feb6aec5e019521ff224fd8.png"><link rel="preload" href="/assets/css/0.styles.0834cda1.css" as="style"><link rel="preload" href="/assets/js/app.8e7b5054.js" as="script"><link rel="preload" href="/assets/js/page--src--templates--blog-vue.2099f288.js" as="script"><link rel="stylesheet" href="/assets/css/0.styles.0834cda1.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
    <meta
      name="google-site-verification"
      content="j7cLUK-kxGEpMOY7valSkamH4kQe_l38yMMAXbpk19U"
    />
    <noscript>
      <div class="noscript-container">
        <a href="/" class="noscript-logo">Tarka <br />_Labs</a>
        <div class="noscript-text-container">
          <h1>Uh oh! Looks like <span>JavaScript</span> is disabled.</h1>
          <p>
            Bitman requires <span>JavaScript</span> to fuel his thirst to end
            bad UX and lazy design, which is necessary to give you the best
            viewing experience possible. Please enable it to continue to our
            website.
          </p>
        </div>
      </div>
    </noscript>
  </head>

  <body >
    <div><div id="app" data-server-rendered="true" data-v-b5a9baaa><div data-v-b5a9baaa><div id="banner-section" data-v-b5a9baaa><div class="banner" style="background:;display:none;" data-v-b5a9baaa><div class="layout" data-v-b5a9baaa><div data-v-b5a9baaa></div><div class="hero heading-size-5" style="color:#ffffff;" data-v-b5a9baaa></div><div data-v-b5a9baaa></div></div></div><div data-v-b5a9baaa></div></div><div class="sidebar" data-v-d6d2017e data-v-b5a9baaa><div class="nav" data-v-d6d2017e><div class="fixed" data-v-d6d2017e><a href="/" data-v-9b1a4c9c data-v-d6d2017e> TARKA<br data-v-9b1a4c9c>_LABS </a><div class="menu" data-v-50705736 data-v-d6d2017e><div class="control" style="stroke:black;" data-v-50705736><div class="spinner heading-size-9" data-v-02ea7230 data-v-50705736><span class="side-menu-symbol" data-v-02ea7230><span class="menu-symbol rotate180t90" data-v-02ea7230><svg height="10" width="10" data-v-02ea7230><line x1="0" y1="5" x2="10" y2="5" style="stroke-width: 2" data-v-02ea7230></line></svg></span><span class="menu-symbol rotate90t0" data-v-02ea7230><svg height="10" width="10" data-v-02ea7230><line x1="0" y1="5" x2="10" y2="5" style="stroke-width: 2" data-v-02ea7230></line></svg></span></span><span style="vertical-align: middle" data-v-02ea7230> menu </span></div><div class="active-page body-jetbrains-size-11 fade-in" data-v-128e5feb data-v-50705736>
  
</div></div><div class="items hidden" data-v-50705736><ul class="main-nav" data-v-748716d7 data-v-50705736><div class="primary fadeOut sidebar" data-v-748716d7><li data-v-748716d7><a href="/genai" data-v-748716d7>/GenAI</a></li><li data-v-748716d7><a href="https://design.tarkalabs.com/" target="_self" data-v-748716d7>/Design</a></li><li data-v-748716d7><a href="/develop" data-v-748716d7>/Develop</a></li><li data-v-748716d7><a href="/case-studies" data-v-748716d7>
        /Case_Studies
      </a></li><li data-v-748716d7><a href="/careers" data-v-748716d7>/Careers</a></li><li data-v-748716d7><a href="/about" data-v-748716d7>/About</a></li><li data-v-748716d7><a href="/contact" data-v-748716d7>/Contact</a></li></div></ul><ul class="sub body-jetbrains-size-11 fadeOut sidebar" data-v-3cc3824e data-v-50705736><li data-v-3cc3824e><a href="/blogs" class="active" data-v-3cc3824e>/blogs</a></li><li data-v-3cc3824e><a href="/talks" data-v-3cc3824e>/talks</a></li><li data-v-3cc3824e><a href="/train" data-v-3cc3824e>/train</a></li></ul></div></div><a href="/contact" class="heading-size-9" data-v-573e12d8 data-v-d6d2017e>
  Get in<br data-v-573e12d8>touch<span class="arrow" data-v-573e12d8>-&gt;</span></a></div></div><!----></div><div class="baseLayout" data-v-996dafb6 data-v-b5a9baaa><div data-v-996dafb6></div><div data-v-996dafb6><div class="blog-details-page" data-v-996dafb6><div class="inner-section" data-v-996dafb6><p class="heading-size-10 capitalize" data-v-996dafb6>
        /AI - 5 min read
      </p><h1 class="title" data-v-996dafb6>Person of Interest: How I built a Facial Recognition Machine Learning model.</h1><div class="author-basic-info" data-v-996dafb6><img src="/assets/img/rajiv.13a78536.jpg" alt="Rajiv Manivannan" width="64" data-v-996dafb6><div data-v-996dafb6><p class="heading-size-9 capitalize" data-v-996dafb6>Rajiv Manivannan</p><p class="body-jetbrains-size-10" data-v-996dafb6>
            Developer
          </p></div></div></div><img src="/assets/img/1.41951ccc.webp" alt="Person of Interest: How I built a Facial Recognition Machine Learning model." class="header-img" data-v-996dafb6><div class="toc-wrapper" data-v-996dafb6><h2 class="heading-size-7" data-v-996dafb6>Table of contents</h2><div class="toc" data-v-996dafb6><a href="#content" class="heading-size-9 heading" data-v-996dafb6><p data-v-996dafb6>/01</p><p data-v-996dafb6>Intro</p></a><a href="#toc-section-1" class="heading-size-9 heading" data-v-996dafb6><p data-v-996dafb6>/02</p><p data-v-996dafb6>Setting up the Work Environment</p></a><a href="#toc-section-2" class="heading-size-9 heading" data-v-996dafb6><p data-v-996dafb6>/03</p><p data-v-996dafb6>Step1 : Collecting data and creating dataset</p></a><a href="#toc-section-3" class="heading-size-9 heading" data-v-996dafb6><p data-v-996dafb6>/04</p><p data-v-996dafb6>Step 2: Training the ML model</p></a><a href="#toc-section-4" class="heading-size-9 heading" data-v-996dafb6><p data-v-996dafb6>/05</p><p data-v-996dafb6>Step 3: Test the ML model with test dataset</p></a><a href="#toc-section-5" class="heading-size-9 heading" data-v-996dafb6><p data-v-996dafb6>/06</p><p data-v-996dafb6>Conclusion</p></a></div></div><div id="content" class="content" data-v-996dafb6><p>At work one day, in a casual conversation with my teammates, the topic of Machine Learning came up. I was instantly hooked, and keenly listened to everyone talking about it.</p>
<p>Someone brought up the TV series ‚Äú<strong><em>Person of Interest</em></strong>‚Äù ‚Äî a sci-fi crime drama, which is actually based on AI. This intrigued me, and made me want to explore more.</p>
<p>Here is the story-line of the TV series ‚Äî a computer programmer develops an application for the government that is capable of collating all sources of information to predict and identify people planning terrorist acts. Based on the predicted information, he tries to stop the crime.</p>
<p>How? ‚Äî The entire city‚Äôs surveillance data feed would be given to the system as input. The machine would collate the relevant information by analysing the video feed, and learn the pattern that occurred in every crime activity.</p>
<p>Cut to a few days later, I came across an interesting project called <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="nofollow noopener noreferrer"><strong>Facenet</strong></a>. This project deals with Face recognition ‚Äì <em>a computer vision task of identifying and verifying a person based on a photograph of their face.</em></p>
<p>After going through the details of this project, <em>I could relate it with the Person Of Interest TV series</em>. I attempted to build the same thing, but by adding my own person of interest image set into the project.</p>
<blockquote>
<p>The process of adding a small dataset with a pre-trained model and performing retraining is called <strong>Transfer Learning</strong>. It is much faster and extremely cost-effective because all the learnings are already done. Adding a new dataset on top of it is just a small effort of learning which will be faster and cost-effective.</p>
</blockquote>
<p>In this post, I will share my experience building the project, and creating the ML model.</p>
<p>Before we dive into building the model, let us understand the steps involved in the building process.</p>
<p>To build a machine learning model, we need data and it has to undergo the following steps:</p>
<ul>
<li>First, the raw data needs to be curated, cleaned and organised as a dataset. In our case, it‚Äôs the set of images of each person. Generally this will be done with the help of data scientists and domain experts.</li>
<li>Second, the dataset needs to be split into two. Training dataset (80%) and Test dataset (20%)</li>
<li>Third, we write our own algorithm or choose an existing machine learning algorithm which is the best fit for our usecase.</li>
<li>Lastly, we need to do the training by adjusting the input values till we get the expected accuracy level.</li>
</ul>
<p>Now, it‚Äôs time to dive deep, into the hands-on üôÉ</p>
<p>I forked the <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="nofollow noopener noreferrer"><strong>Facenet</strong></a> project into my Github account to do my experiments. Also, I containerised it using <a href="https://www.docker.com/" target="_blank" rel="nofollow noopener noreferrer"><strong>Docker</strong></a> for the faster start up and easier management. These two steps are of course optional, but I found that it made things very convenient for me.</p>
<p>Once this is done, here is how you too can go about building your version of this application:</p>
<h1 id="toc-section-1"><a href="#setting-up-the-work-environment" aria-hidden="true"><span class="icon icon-link"></span></a>Setting up the Work Environment</h1>
<p>Ensure that Git and Docker installed on your machine.</p>
<p>Open the terminal and clone the project into you workspace</p>
<div class="gridsome-highlight" data-language="shell"><pre class="language-shell"><code class="language-shell">$ <span class="token function">git</span> clone https://github.com/rajivmanivannan/facenet
$ <span class="token builtin class-name">cd</span> facenet</code></pre></div>
<p>Use the below docker compose command to build the docker image.</p>
<div class="gridsome-highlight" data-language="shell"><pre class="language-shell"><code class="language-shell">$ <span class="token function">docker-compose</span> build</code></pre></div>
<figure><img src="/blog/building-a-facial-recognition-machine-learning-model/2.webp" alt="Docker image build successfully"><figcaption>Docker image build successfully</figcaption></figure>
<p>To run the dockerized application execute the below command</p>
<div class="gridsome-highlight" data-language="shell"><pre class="language-shell"><code class="language-shell">$ <span class="token function">docker-compose</span> run <span class="token parameter variable">--rm</span> cli <span class="token function">bash</span></code></pre></div>
<figure><img src="/blog/building-a-facial-recognition-machine-learning-model/3.webp" alt="Docker image started and running"><figcaption>Docker image started and running</figcaption></figure>
<p>Run the below commands to create required directory for our work.</p>
<div class="gridsome-highlight" data-language="shell"><pre class="language-shell"><code class="language-shell">$ <span class="token function">mkdir</span> models <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> datasets/lfw/raw <span class="token function">mkdir</span> <span class="token parameter variable">-p</span>
datasets/my_dataset/<span class="token punctuation">{</span>train,test<span class="token punctuation">}</span></code></pre></div>
<img src="/blog/building-a-facial-recognition-machine-learning-model/4.webp">
<h1 id="toc-section-2"><a href="#step1--collecting-data-and-creating-dataset" aria-hidden="true"><span class="icon icon-link"></span></a>Step1 : Collecting data and creating dataset</h1>
<p>I collected images of myself and one celebrity from internet. I then organized it in two separate folders. There were around 15 image set in each folder. Then, I moved the folders into the following path <em>/app/datasets/lfw/raw</em>.</p>
<figure><img src="/blog/building-a-facial-recognition-machine-learning-model/5.webp" alt="Collected images (raw data) for training"><figcaption>Collected images (raw data) for training</figcaption></figure>
<p>In this project they have used the Multi-task Cascaded Convolutional Neural Networks (<a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html" target="_blank" rel="nofollow noopener noreferrer">MTCNN</a>) to detect the face from the image and crop it into 160x160 images. Aligned and cropped images (the dataset) will be stored in <em>/datasets/lfw/lfw_mtcnnpy_160</em>. To create the dataset run the following command.</p>
<div class="gridsome-highlight" data-language="shell"><pre class="language-shell"><code class="language-shell"><span class="token keyword">for</span> <span class="token for-or-select variable">N</span> <span class="token keyword">in</span> <span class="token punctuation">{</span><span class="token number">1</span><span class="token punctuation">..</span><span class="token number">4</span><span class="token punctuation">}</span><span class="token punctuation">;</span> <span class="token keyword">do</span> python src/align/align_dataset_mtcnn.py
/app/datasets/lfw/raw /app/datasets/lfw/lfw_mtcnnpy_160 <span class="token parameter variable">--image_size</span>
<span class="token number">160</span> <span class="token parameter variable">--margin</span> <span class="token number">32</span> --random_order<span class="token punctuation">;</span> <span class="token keyword">done</span></code></pre></div>
<figure><img src="/blog/building-a-facial-recognition-machine-learning-model/6.webp" alt="Aligned using [MTCNN](https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html) (Now it‚Äôs called dataset)"><figcaption>Aligned using <a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html">MTCNN</a> (Now it‚Äôs called dataset)</figcaption></figure>
<h1 id="toc-section-3"><a href="#step-2-training-the-ml-model" aria-hidden="true"><span class="icon icon-link"></span></a>Step 2: Training the ML model</h1>
<p>We need to split the dataset ‚Äî 80% to Train and 20% for Test. I took 3 images as test set form 15 images and kept aside. Then placed the dataset into the train and test folder in the following path <em>/datasets/my_dataset/train.</em></p>
<p>And then, download the pre-trained model <a href="https://drive.google.com/open?id=1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-" target="_blank" rel="nofollow noopener noreferrer">20180402‚Äì114759</a>, extract it into the models folder in the project. This model has been trained on the <a href="https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/" target="_blank" rel="nofollow noopener noreferrer">VGGFace2</a> dataset consisting of ~3.3M faces and ~9000 identities (classes)<em>.</em></p>
<p>Run the below command to train and build your own model with your dataset.</p>
<div class="gridsome-highlight" data-language="shell"><pre class="language-shell"><code class="language-shell">python src/classifier.py TRAIN ./datasets/my_dataset/train/
./models/20180402-114759.pb ./models/my_classifier.pb <span class="token parameter variable">--batch_size</span>
<span class="token number">1000</span></code></pre></div>
<figure><img src="/blog/building-a-facial-recognition-machine-learning-model/7.webp" alt="Trained and created my_classifer.pb ML Model"><figcaption>Trained and created my_classifer.pb ML Model</figcaption></figure>
<p>After the completion of the training it will create a new model with our dataset named <strong>my_classifer.pb.</strong> You can find in the path /models.</p>
<h1 id="toc-section-4"><a href="#step-3-test-the-ml-model-with-test-dataset" aria-hidden="true"><span class="icon icon-link"></span></a>Step 3: Test the ML model with test dataset</h1>
<p>Now, it‚Äôs time to test our model with test dataset. Place the test dataset in the following path <em>/datasets/my_dataset/test.</em> Run the below command to classify the test data against our model. It will provide the face match accuracy percentage of each image.</p>
<div class="gridsome-highlight" data-language="shell"><pre class="language-shell"><code class="language-shell">python src/classifier.py CLASSIFY ./datasets/my_dataset/test/
./models/20180402-114759.pb ./models/my_classifier.pb <span class="token parameter variable">--batch_size</span>
<span class="token number">1000</span></code></pre></div>
<figure><img src="/blog/building-a-facial-recognition-machine-learning-model/8.webp" alt="Accuracy of test dataset with actual person‚Äôs image."><figcaption>Accuracy of test dataset with actual person‚Äôs image.</figcaption></figure>
<h1 id="toc-section-5"><a href="#conclusion" aria-hidden="true"><span class="icon icon-link"></span></a>Conclusion</h1>
<p>I believe this provides the core piece to build a full-fledged facial recognition based application. I am working on building an App to recognise my teammates and greet them by name.Will keep you posted on how it goes!</p>
</div><hr data-v-996dafb6><div class="inner-section" data-v-996dafb6><div class="author-basic-info" data-v-996dafb6><img src="/assets/img/rajiv.13a78536.jpg" alt="Rajiv Manivannan" width="64" data-v-996dafb6><div data-v-996dafb6><p class="heading-size-9 capitalize" data-v-996dafb6>Rajiv Manivannan</p><p class="body-jetbrains-size-10" data-v-996dafb6>
            Developer
          </p></div></div><p class="body-opensans-size-9" data-v-996dafb6>He is still thinking what to write about him</p><div class="social-links" data-v-996dafb6><!----><!----><!----></div></div><hr data-v-996dafb6><div class="related-articles" data-v-996dafb6><div class="flex-space-between" data-v-996dafb6><p class="heading-size-7" data-v-996dafb6>_related articles</p><a href="/blogs" class="heading-size-9 underline" data-v-996dafb6>View all</a></div><div class="flex-space-between blog-card-wrapper" data-v-996dafb6><a href="/blogs/extracting-structured-data" class="blog-card" data-v-996dafb6><img src="/assets/img/default-cover.55254b66.png" alt="Extracting Structured Data from PDFs with Claude Sonnet and Amazon Bedrock" class="thumbnail" data-v-996dafb6><div class="info" data-v-996dafb6><p class="heading-size-10 capitalize" data-v-996dafb6>/AI</p><h3 data-v-996dafb6>Extracting Structured Data from PDFs with Claude Sonnet and Amazon Bedrock</h3><p class="body-jetbrains-size-10" data-v-996dafb6>
              Kesavan ¬† 7 min read
            </p></div></a><a href="/blogs/batch-transcription" class="blog-card" data-v-996dafb6><img src="/assets/img/batch-transcription.e03c0d91.webp" alt="Implementing Serverless Batch Transcription with AWS Step Functions and Azure AI Services" class="thumbnail" data-v-996dafb6><div class="info" data-v-996dafb6><p class="heading-size-10 capitalize" data-v-996dafb6>/AI</p><h3 data-v-996dafb6>Implementing Serverless Batch Transcription with AWS Step Functions and Azure AI Services</h3><p class="body-jetbrains-size-10" data-v-996dafb6>
              Kesavan ¬† 8 min read
            </p></div></a></div></div><div class="lets-talk" data-v-188e6a71><div class="message heading-size-7" data-v-188e6a71>
    Let‚Äôs build digital solutions together.
  </div><div class="action-container" data-v-188e6a71><div class="action-button heading-size-5" data-v-188e6a71><div data-v-188e6a71>Get in touch</div><div class="arrow" data-v-188e6a71>-&gt;</div></div><img src="/assets/img/talk.71d1d2f3.svg" alt="Lenny Face" data-v-188e6a71></div></div></div></div><div data-v-996dafb6></div></div><div id="footer" class="baseLayout" data-v-996dafb6 data-v-b5a9baaa><div data-v-996dafb6></div><div data-v-996dafb6><div class="site-footer" data-v-0a037a4b data-v-b5a9baaa><ul class="main-nav footer" data-v-748716d7 data-v-0a037a4b><div class="primary fadeIn" data-v-748716d7><li data-v-748716d7><a href="/genai" data-v-748716d7>/GenAI</a></li><li data-v-748716d7><a href="https://design.tarkalabs.com/" target="_self" data-v-748716d7>/Design</a></li><li data-v-748716d7><a href="/develop" data-v-748716d7>/Develop</a></li><li data-v-748716d7><a href="/case-studies" data-v-748716d7>
        /Case_Studies
      </a></li><li data-v-748716d7><a href="/careers" data-v-748716d7>/Careers</a></li><li data-v-748716d7><a href="/about" data-v-748716d7>/About</a></li><li data-v-748716d7><a href="/contact" data-v-748716d7>/Contact</a></li></div></ul><div class="subnav" data-v-0a037a4b><ul class="sub body-jetbrains-size-11 footer fadeIn" data-v-3cc3824e data-v-0a037a4b><li data-v-3cc3824e><a href="/blogs" class="active" data-v-3cc3824e>/blogs</a></li><li data-v-3cc3824e><a href="/talks" data-v-3cc3824e>/talks</a></li><li data-v-3cc3824e><a href="/train" data-v-3cc3824e>/train</a></li></ul><div data-v-fae945d8 data-v-0a037a4b><a href="https://twitter.com/tarkalabs" target="_blank" rel="noopener" class="external" data-v-fae945d8>Twitter</a><a href="https://in.linkedin.com/company/tarka-labs" target="_blank" rel="noopener" class="external" data-v-fae945d8>LinkedIn</a></div><div class="sedin" data-v-0a037a4b><a href="https://sedintechnologies.com/" target="_blank" rel="noopener" class="external" data-v-0a037a4b>Tarka Labs is a division of Sedin Technologies</a></div></div></div></div><div data-v-996dafb6></div></div></div></div></div>
    <script src="/assets/js/app.8e7b5054.js" defer></script><script src="/assets/js/page--src--templates--blog-vue.2099f288.js" defer></script><script data-vue-tag="ssr" src="https://unpkg.com/@lottiefiles/lottie-player@0.4.0/dist/lottie-player.js" data-body="true"></script>
  </body>
</html>
